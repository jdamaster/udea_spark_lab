# Data Pipelines with spark and Databricks
An Introduction to the Data Engineering role 

## Environment configuration

First of all we need an environment to run our code, we will create a free Databricks community account [Link to community site](https://docs.databricks.com/en/getting-started/community-edition.html), then verify your address and access to the console

Create a new Compute Resource
Go into the compute section of the left pannel and create a simple compute resource within the default parameters 

![Compute 1](assets/readme/1_1_compute_resource_creation.jpg)

Select the 12.2LTS Databricks Runtime
![Compute 2](assets/readme/1_2_compute_resource_creation.jpg)

Wait until the cluster creation completes
![Compute 3](assets/readme/1_3_compute_resource_creation.jpg)
At the end we will have our cluster ready to code

In the meantime, What If we create our data warehouse?
We will store the data generated by our ETL on Snowflake, Which is a popular Data Lake cloud solution
First, lets go into their main page [Snowflake](https://www.snowflake.com/en/)
![Snowflake 1](assets/readme/3_1_snowflake_home.jpg)

Create a trial account, you will have 30 days to play with the platform
![Snowflake 2](assets/readme/3_2_snowflake_user_creation.jpg)

You should find yourself in the mainpage, there are several tools and learning resources in here, but we will explore in deep later
![Snowflake 3](assets/readme/3_3_snowflake_mainpage.jpg)

Lets create a database, by default It will include a default schema, which will be empty
![Snowflake 4](assets/readme/3_4_snowflake_db_creation.jpg)

Lets jump back to Databricks
Now there we are, lets create a new Jupyter notebook the Jupyter Notebook
![Jupyter 1](assets/readme/2_1_notebook_creation.jpg)


Import the **Spark ETL Demo.ipynb** Jupyter notebook from the Notebooks folder
![Jupyter 2](assets/readme/2_2_notebook_creation.jpg)

Connect the notebook to the compute resource you just create
![Jupyter 3](assets/readme/2_3_notebook_creation.jpg)
And we are ready to go!



